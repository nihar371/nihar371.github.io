{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ccb82b3",
   "metadata": {},
   "source": [
    "# Web Scraping with Selenium\n",
    "Scraping NEWS articles using Selenium from CNBC's Website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb81db1",
   "metadata": {},
   "source": [
    "**Data collection!!!**\n",
    "\n",
    "*\"Collecting\"* and *\"organizing data into a proper format\"* can be very tedious, and time-consuming. You can imagine that the internet is a massive pool of data and what you want to do is extract a very tiny amount of data that is relevant and easy to work with for your task.\n",
    "\n",
    "Ok, so now we need to collect data, but HOW ??? Data from the internet can be accessed or downloaded through various such as simply by downloading, or by API calls, and can be many more, and from these one of them is by scraping.\n",
    "\n",
    "<center>\n",
    "    <figure>\n",
    "    <img src=\".\\\\web-scraping-with-selenium\\\\images\\\\ice-scraping-winter.gif\" alt=\"Scraping in reality\">\n",
    "    <figcaption style=\"font-style: italic;\">Yeap, this is what Scraping in reality looks like...XD</figcaption>\n",
    "    </figure>\n",
    "</center>\n",
    "\n",
    "We will be doing the same but virtually. We will be scrapping news articles from CNBC's Website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c45012e",
   "metadata": {},
   "source": [
    "## But WAIT, HOLD ON! Why Selenium only, there would be other tools too to work with, right?\n",
    "\n",
    "Yes, definitely. Some of the most popular libraries are [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) and [Scrapy](https://scrapy.org/). With these libraries, it is easier to fetch the structure of the websites, but they are not well suited in case we want to perform various actions after receiving the HTML contents of the page, such as navigating, scrolling, filling forms, taking screenshots, and executing JavaScript. Selenium enables all of these features at ease as it provides a UI interface of the browser that loads all the HTML content in it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793694cc",
   "metadata": {},
   "source": [
    "**For installing Selenium**:\n",
    "\n",
    "\n",
    "<blockquote>\n",
    "Install Selenium package:\n",
    "\n",
    "```pip install selenium```\n",
    "\n",
    "Download the web driver based on your type of browser and its version:<br>\n",
    "*https://www.selenium.dev/ecosystem/*\n",
    "\n",
    "</blockquote> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817a6940",
   "metadata": {},
   "source": [
    "## Selected the Tool… NOW let's move forward towards our GOAL. SCRAPE IT!!!\n",
    "\n",
    "OK, so here is our website CNBC : 'https://www.cnbc.com/', first of all we will see whether we can scrape it or not. Websites provide a list of permissions in the ```robots.txt```. You can see those permissions by appending *\"/robots.txt\"* at the end of the URL. So, in our case, it will be https://www.cnbc.com/robots.txt.\n",
    "\n",
    "<center>\n",
    "    <img src='.\\\\web-scraping-with-selenium\\\\images\\\\scraping-permission-using-robots.gif' alt='GIF for Scraping permissions using robots.txt'></img>\n",
    "    <figcaption style=\"font-style: italic;\">Scraping permissions using robots.txt</figcaption>\n",
    "</center>\n",
    "\n",
    "CNBC provides all the permissions to scrape as a user-agent except */preview*, */undefined*, */proplayer*, *appchart/\\**, and */search/\\**.\n",
    "\n",
    "Coming back to the home page. We can see a ton of information present starting from headlines, market movers, latest news, also including their promotion for CNBC+, special reports, business news, political section, and many more sections. But that does not fulfill our criteria of getting a large data source of articles of the same type, as there is also information that is not important to us.\n",
    "\n",
    "So, again we will look out for a page that contains all the relevant reports on the CNBC website. If we navigate into the Economy, Finance, or Technology section present in the Market drop-down option in the menu bar. Then we will be directed to a new page that contains all the articles based on the category, which makes our life a lot easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50119f35",
   "metadata": {},
   "source": [
    "## Let's get the basics done…\n",
    "\n",
    "One of the most basic but most important things is to locate a specific tag in HTML as efficiently as possible. There are different ways to obtain a tag using Selenium. They are either based on their ID, Tag name, Class name, CSS Selector, or XPath or by using a combination of these.\n",
    "\n",
    "<center>\n",
    "\n",
    "<pre>\n",
    "╔════════════════╦══════════════════════════════════════════════════════╗\n",
    "║     Finders    ║         Selenium WebDriver Command (Python)          ║\n",
    "╠════════════════╬══════════════════════════════════════════════════════╣\n",
    "║                ║ driver.find_element(By.ID, \"id\")                     ║\n",
    "║                ║ driver.find_element(By.TAG_NAME, \"tag\")              ║\n",
    "║  find_elemenet ║ driver.find_element(By.CLASS_NAME, \"class\")          ║\n",
    "║                ║ driver.find_element(By.CSS_SELECTOR, \"css selector\") ║\n",
    "║                ║ driver.find_element(By.XPATH, \"path\")                ║\n",
    "╚════════════════╩══════════════════════════════════════════════════════╝\n",
    "</pre>\n",
    "</center>\n",
    "\n",
    "One thing to remember is that *\"find_element\"* will only return the collection of the first tag that satisfies the condition. Most of the time you will need all the tags that satisfy the condition, so for that a similar function *\"find_elements\"* is used. It will return a list of collections, and in case if there isn't a match found then it will return an empty list.\n",
    "\n",
    "So, we will identify a common pattern in the tags we are interested in extracting.\n",
    "\n",
    "You can refer to these links [Locators](https://www.selenium.dev/documentation/webdriver/elements/locators/), and [Finders](https://www.selenium.dev/documentation/webdriver/elements/finders/) for more information.\n",
    "\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "`/`: root of the document\n",
    "\n",
    "`//`: start from anywhere in the document\n",
    "\n",
    "`/html/body//p`: retrieve all `p` tags under `html/body` from the root\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024ff550",
   "metadata": {},
   "source": [
    "## Back to the Business of Scraping...\n",
    "\n",
    "Let's observe our page and its HTML structure.\n",
    "\n",
    "We can divide our page into 3 views, Top, Middle, and Bottom view.\n",
    "\n",
    "**Top View:**\n",
    "\n",
    "<ol>\n",
    "  <li>Every article is in a different size of a card.</li>\n",
    "  <li>Every article's URL is present as a hyperlink in the title of the card.</li>\n",
    "  <li>There are also Paid Posts present on the page in the form of articles.</li>\n",
    "  <li>There are also advertisements present, on the right side of the page.</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src='.\\\\web-scraping-with-selenium\\\\images\\\\top-page-preview.png' alt='Top Page Preview image'></img>\n",
    "    <figcaption style=\"font-style: italic;\">Top page Preview</figcaption>\n",
    "</center>\n",
    "\n",
    "**Middle View:**\n",
    "\n",
    "<ol>\n",
    "  <li>Also contains advertisement blocks.</li>\n",
    "  <li>Contains a sub-section of articles highlighting Trending News.</li>\n",
    "</ol>\n",
    "\n",
    "<center>\n",
    "    <img src='.\\\\web-scraping-with-selenium\\\\images\\\\middle-page-preview.png' alt='Middle page Preview image'></img>\n",
    "    <figcaption style=\"font-style: italic;\">Middle page Preview</figcaption>\n",
    "</center>\n",
    "\n",
    "**Bottom View:**\n",
    "\n",
    "<ol>\n",
    "  <li>Contains a \"Load More\" button which enables more articles to load on this page, without any redirecting.</li>\n",
    "  <li>And some more ADS.</li>\n",
    "</ol>\n",
    "\n",
    "<center>\n",
    "    <img src='.\\\\web-scraping-with-selenium\\\\images\\\\bottom-page-preview.png' alt='Bottom page Preview image'></img>\n",
    "    <figcaption style=\"font-style: italic;\">Bottom page Preview</figcaption>\n",
    "</center>\n",
    "\n",
    "Ok, so now we know what all things are present in the page and what we need to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff80208",
   "metadata": {},
   "source": [
    "## So, here is the plan…\n",
    "\n",
    "<ol>\n",
    "  <li>First of all, we will load all the articles present in this page by clicking on the \"Load More\" button as much as we can until there is no \"Load More\" button.</li>\n",
    "  <li>And then we will see how to extract information from each card, and what to extract.</li>\n",
    "</ol>\n",
    "\n",
    "So, first of all we will have to make an instance of our selenium web-driver so that we can load our CNBC's website into it.\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "```python\n",
    "cnbc_eco_url = 'https://www.cnbc.com/economy/'\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(cnbc_eco_url)\n",
    "```\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "Ok, so at this point our browser has loaded the website, now we will have to find the \"Load Button\". But right now it's not visible on our screen right. So, we will have to scroll down until we can see our button.\n",
    "\n",
    "There are several ways to deal with this:\n",
    "\n",
    "<ul>\n",
    "<li>Using JavaScript:</li>\n",
    "\n",
    "Learn more how to implement from [Scrapfy](https://scrapfly.io/blog/how-to-scroll-to-the-bottom-with-selenium/).\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "```python\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "\n",
    "<li>Using Action Chains:</li>\n",
    "\n",
    "Learn more how to implement from [Selenium](https://www.selenium.dev/documentation/webdriver/actions_api/wheel/).\n",
    "<blockquote>\n",
    "\n",
    "```python\n",
    "ActionChains(driver)\\\n",
    "        .scroll_to_element(button)\\\n",
    "        .perform()\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "\n",
    "<li>Using Traditional Page-Down Key:</li>\n",
    "\n",
    "Learn more how to implement from [Selenium](https://www.selenium.dev/documentation/webdriver/actions_api/keyboard/#key-down).\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "```python\n",
    "body.send_keys(Keys.PAGE_DOWN)\n",
    "```\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4050b071",
   "metadata": {},
   "source": [
    "After loading all the articles, in the page, we will now proceed to collect all the links of the articles from each card.\n",
    "\n",
    "<center>\n",
    "    <img src='.\\\\web-scraping-with-selenium\\\\images\\\\inspecting-card-element.png' alt='Inspecting Card element image'></img>\n",
    "    <figcaption style=\"font-style: italic;\">Inspecting Card element</figcaption>\n",
    "</center>\n",
    "\n",
    "As we can see a common thing among all the article cards is that they have a common unique identifier which is *data-test = \"Card\"*, which makes our search easier. We can get a list of all tags having identifier *data-test = \"Card\"*.\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "```python\n",
    "cards = web_driver.find_elements(By.CSS_SELECTOR, '[data-test=\"Card\"]')\n",
    "```\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "Now let's see which tag we have to extract from each Card.\n",
    "\n",
    "<center>\n",
    "    <img src='.\\\\web-scraping-with-selenium\\\\images\\\\html-structure-tree.png' alt='HTML structure tree of the Card element image'></img>\n",
    "    <figcaption style=\"font-style: italic;\">HTML structure tree of the Card element</figcaption>\n",
    "</center>\n",
    "\n",
    "All the information in which we are interested is present in the *\\<a\\>* tag having *class = \"Card-title\"*.\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "```python\n",
    "<a href=\"https://www.cnbc.com/2024/09/27/pce-inflation-august-2024.html\" class=\"Card-title\" target=\"\">Key Fed inflation gauge at 2.2% in August, lower than expected</a>\n",
    "```\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "Now, it's just a matter of iterating over all the tags present in the list, and getting the URL of those articles. In the case of Paid Post articles, we can filter those articles out by verifying whether the term \"Paid Post\" is present in the \"Card-eyebrow\" tag, which shows the category of the post.\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "```python\n",
    "try:\n",
    "  if 'Paid Post' not in card.find_element(By.CLASS_NAME, 'Card-eyebrow').text:\n",
    "    card_url = card.find_element(By.CLASS_NAME, 'Card-title').get_attribute('href')\n",
    "    url_record.append(card_url)\n",
    "  except:\n",
    "    pass\n",
    "```\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "In the end, we can convert our list into a DataFrame, for better manageability.\n",
    "\n",
    "<center>\n",
    "    <img src='.\\\\web-scraping-with-selenium\\\\images\\\\dataframe-of-all-article-links.png' alt='Dataframe of all article links image'></img>\n",
    "    <figcaption style=\"font-style: italic;\">DataFrame containing all the Article links</figcaption>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d4cff6",
   "metadata": {},
   "source": [
    "## The Show begins now…\n",
    "\n",
    "Now, that we have all the links for all the articles, we can load each link and extract whatever details we want to collect from each article.\n",
    "So, what are you waiting for, now you know all the basics, so go get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59359019",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<ol>\n",
    "    <li><a href='https://www.selenium.dev/documentation/'>The Selenium Browser Automation Project</a></li>\n",
    "    <li><a href='https://scrapfly.io/blog/how-to-scroll-to-the-bottom-with-selenium/'>How to scroll to the bottom of the page with Selenium?</a></li>\n",
    "    <li><a href='https://yoksel.github.io/html-tree/en/'>HTML Tree Generator</a></li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
